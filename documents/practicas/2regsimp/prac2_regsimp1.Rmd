---
title: "Practica 2: Ajuste, Residuos y Correlación"
subtitle: "Estadistica Multivariada - Sociología FACSO Universidad de Chile"
author: "Juan Carlos Castillo & Alejandro Plaza"
output:
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introducción

En esta práctica se continua con ejercicios de regresión simple, enfocados en ajuste, residuos y relación con correlación. Están basados principalmente  en el ejemplo de Darlington & Hayes cap. 2 (The simple regression model).

# Datos

Los datos a utilizar son los mismos que los de la práctica 1, corresponden a un ejemplo ficticio de 23 casos (individuos) y sus datos en dos variables relacionadas con un juego: Las dos variables de esta base de datos son el número de veces que se ha jugado antes (X) y el número de puntos ganados (Y). El archivo de datos original es golf.txt.

```{r}
datos<- read.csv("https://juancarloscastillo.github.io/metsoc-facsouchile/documents/practicas/1_regsimp/golf.txt", sep="")
```

Y generamos un gráfico para recordar la distribución de los datos:

```{r}
pacman::p_load(ggplot2,plotly)

# basic scatterplot
g=ggplot(datos, aes(x=juegos, y=puntos)) +
  geom_point()

gg=ggplotly(g)
gg

```

## Residuos

En el gráfico anterior vemos que la línea resume la relación entre X e Y, pero claramente es una simplificación que no abarca toda la variabilidad de los datos. Por ejemplo, para el sujeto cuya experiencia es haber jugado 1 vez y luego gana 3 puntos, esta línea predice exactamente su puntaje basada en su experiencia. Sin embargo, el sujeto que ha jugado 3 veces y saca 6 puntos se encuentra más lejos de la línea y por lo tanto esta línea o "modelo predictivo" no representa tan bien su puntaje. A esto se refieren los *residuos*, que es la diferencia entre el valor predicho (o $\widehat{Y}$) y el observado $Y$. Por lo tanto, la mejor recta será aquella que minimice al máximo los residuos.

El sentido de la recta que resume de mejor manera la relación entre dos variables es que minimice la suma de todos los residuos. Para realizar la suma de los residuos estos se elevan al cuadrado, lo que se denomina suma de residuos al cuadrado o $SS_{residual}$ ya que como hay residuos positivos y negativos unos se cancelan a otros y la suma es 0. De la infinita cantidad de rectas que se pueden trazar, siempre hay una que tiene un valor menor de $SS_{residual}$. Este procedimiento es el que da nombre al proceso de estimación: residuos cuadrados ordinarios, o *OLS* (Ordinary Least Squares).

## Modelo y cálculo de parámetros

El modelo de regresión entonces se relaciona con una ecuación de la recta, o recta de regresión, que se puede definir en términos simples de la siguiente manera:

$$\widehat{Y}=b_{0} +b_{1}X $$


```{r}
reg1 <-lm(puntos ~juegos, data = datos)
reg1
```

En el formato de la redacción en RMarkdown se pueden presentar los valores de las estimaciones con distintas funciones del paquete **texreg**. Una de ellas es `screenreg`, que ofrece una salida simple en la pantalla:

```{r warning=FALSE, message=FALSE}
library(texreg)

screenreg(reg1, custom.model.names = "Variable dependiente: Puntos de Tacataca", # Cambiar nombre del título
       custom.coef.names = c("Intercepto", "Juegos"),   #Cambiar nombre de las variables
       custom.note = "Elaboración propia en base a Golf.txt", #Agregar una nota
       single.row = T #para que la DS este en una linea
       )
```

Con la función `htmlreg` es posible generar también una tabla que puede ser publicable en formato html, tal como en el siguiente ejemplo:

```{r warning=FALSE, message=FALSE, results='asis'}
htmlreg(reg1, custom.model.names = "Variable dependiente: Puntos de Tacataca", # Cambiar nombre del título
       custom.coef.names = c("Intercepto", "Juegos"),   #Cambiar nombre de las variables
       custom.note = "Elaboración propia en base a Golf.txt", #Agregar una nota
       single.row = T #para que la DS este en una linea
       )
```


En el contexto de los modelos de regresión, las estimaciones pueden presentarse en un gráfico de coeficientes o **coefplot**. 


```{r}
pacman::p_load(coefplot)
coefplot(reg1)

#Modificaciones con ggplot2
coefplot(reg1)+
  scale_y_discrete(name=" ", limits=c("(Intercept)","juegos"),
                   labels=c("Intercepto", "Juegos")) +
  scale_x_continuous(name = " ") +
  ggtitle("Gráficos de Coeficientes Beta")+
  theme_bw()
```



# Bondad de Ajuste: Residuos y $R_{2}$

A partir del método de Mínimos Cuadrados Ordinarios obtenemos una recta que describe un conjunto de datos minimizando las diferencias entre el modelo ajustado a los datos y los datos mismos.

No obstante, incluso cuando se ajusta el mejor modelo puede existir cierta imprecisión, la cual es representada por las diferencias entre los datos observados y los valores predichos por la recta de regresión.

La imprecisión implica evaluar la **Bondad de Ajuste** y se evalua a partir del estadístico $R^2$.


En el siguiente apartado se puede observar la manera de cálcular la predicción de Y (puntos) en base a X (juegos), y almacenarlos en la base de datos, con los respectivos residuos.

```{r}

#summary(lm(puntos~juegos, data=datos))
#beta=0.5 intercepto=2.5

#Variable de valores predichos
datos$estimado<- (2.5 + datos$juegos*0.5)

# Alternativa por comando
#datos$estimado <- predict(reg1)

#Estimamos el residuo
datos$residuo <- datos$puntos - datos$estimado

# Alternativa por comando
#datos$residuo <- residuals(reg1)

datos %>% select(id, estimado, residuo) %>% head()

```

## Suma de cuadrados y $R^{2}$

Usando la media como modelo podemos calcular las diferencias entre los valores observados y los valores predichos por la media.

  + La suma de las diferencias al cuadrado la llamamos **Suma Total de Cuadrados**:

$$SS_{tot} = \sum(y-\bar{y})^2 $$
Y calculamos

```{r}
ss_tot<- sum((datos$puntos-mean(datos$puntos))^2); ss_tot
```


Usando la predicción del modelo OLS podemos llegar a tener una mejor aproximación:

  + la suma de las diferencias entre los datos observados y los datos predichos al cuadrado le llamamos **Suma de residuos al cuadrado**:

$$SS_{error} = \sum(y-\hat{y})^2$$
Este valor representa el grado de imprecisión cuando la mejor recta se ajusta a los datos.

```{r}

ss_err<- sum((datos$puntos - datos$estimado)^2);ss_err

```

Cuánto mejor ajusta un modelo OLS sobre el cálculo de la media. Esto se puede estimar la diferencia al cuadrado entre el valor estimado y la media, esto se llama **Suma explicada de cuadrados**

$$SS_{reg} = \sum(\hat{y}-\bar{y})^2$$
```{r}

ss_reg<-sum((datos$estimado-mean(datos$puntos))^2) ; ss_reg

```

A partir de la suma de cuadrados es posible calcular el estadístico $R^{2}$

$$R^2=\frac{SS_{reg}}{SS_{tot}}= 1- \frac{SS_{error}}{SS_{tot}}$$
```{r}
#Opción 1
ss_reg/ss_tot
#Opción 2
1-ss_err/ss_tot
#por comando
summary(lm(puntos~juegos, data=datos))$r.squared

```



## Visualización

En la siguiente sección se presentan distintas formas de visualizar los residuos a partir del paquete **ggplot2**.

```{r}
#Visualizacion
library(ggplot2)

ggplot(datos, aes(x=juegos, y=puntos))+
  geom_smooth(method="lm", se=FALSE, color="lightgrey") +#Pendiente de regresion
  geom_segment(aes(xend=juegos, yend=estimado), alpha = .2) + #Distancia entre estimados y datos en lineas
  geom_point() + #Capa 1
  geom_point(aes(y=estimado), shape =1) +
  theme_bw()



#Opción donde se juega con el tamaño de los puntos y los colores para resalta el tamaño de los residuos.

ggplot(datos, aes(x=juegos, y=puntos))+
  geom_smooth(method="lm", se=FALSE, color="lightgrey") +#Pendiente de regresion
  geom_segment(aes(xend=juegos, yend=estimado), alpha = .2) + #Distancia entre estimados y datos en lineas
  geom_point(aes(color = abs(residuo), size = abs(residuo))) +
  scale_color_continuous(low = "black", high = "red") +
  guides(color = FALSE, size = FALSE) +
   geom_point(aes(y=estimado), shape =1) +
  theme_bw()



```


# El coeficiente de Regresión versus el coeficiente de correlación

tanto $r_{xy}$ y $\beta_1$ son medidas de la relación entre X e Y. Ellas estan relacionadas con la formula de:

$$\beta_1= r_{xy}(S_y/S_x)$$

Es decir:
```{r}
beta<-cor(datos$juegos,datos$puntos)*(sd(datos$puntos)/sd(datos$juegos));beta

reg1$coefficients[2] #llamamos al coeficiente beta (en posición 2) en el objeto reg1


```

Del mismo modo existe una relación entre $r_{xy}$ y $R^2$

```{r}

#Correlación (Pearson) entre juegos y puntos (r)
cor(datos$juegos,datos$puntos)
#Correlación entre juegos y puntos al cuadrado.
(cor(datos$juegos,datos$puntos)
)^2


```

Las diferencias entre la Regressión y la Correlación se puede expresar en términos de formulas. Por un lado se puede entender que $r_{xy}$ es una forma estandarizada de $\beta$, como se puede apreciar en el siguente apartado.

```{r}

#Para ahorrarnos el tipeo de datos, lo adjuntamos al espacio de trabajo

attach(datos)


Xpuntos_Z <- (puntos-(mean(puntos)))/sd(puntos)
Yjuegos_Z<- (juegos-(mean(juegos)))/sd(juegos)

lm(Xpuntos_Z~Yjuegos_Z)$coefficients


```

En términos de las propiedades, el r de Pearson no es influenciado por la escala de medición mientras que en el beta si hay modificaciones:

Con los datos que hemos trabajado podemos corroborar lo anterior. A continuación se muestra como los puntos al dividirlos por 100, en la correlación se mantiene la relación; pero esto no ocurre así con la regresión.

```{r}

cor(juegos,puntos/100)
X_reescalado <- juegos/100
lm(puntos~X_reescalado)$coefficients

```



La correlación entre X e Y es la misma que entre Y e X,
```{r}
cor(juegos,puntos)
cor(puntos,juegos)

```


La regresión etre X e Y **no** es la misma que entre Y e X

```{r}
lm(puntos~juegos)$coefficients
lm(juegos~puntos)$coefficients

```
